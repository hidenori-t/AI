# 参考文献
岡谷貴之 :『深層学習』講談社, 2015.

# 1.1.2
# autoencoder (自己符号化器)
入力に対し計算される出力が,入力になるべく近くなるように訓練されるニューラルネット.

# 2.1
# feedforward neural network : FFNN (順伝播型ニューラルネットワーク)
層状に並べたユニットが隣接層間でのみ結合した構造を持ち,情報が入力側から出力側に一方向にのみ伝播するニューラルネットワーク.
文献によっては multi-layer perceptron(多層パーセプトロン)と呼ばれる.

# 2.2 activation function (活性化関数)
古くから最もよく使われているのが,
## logistic sigmoid function (ロジスティックシグモイド関数) あるいは logistic function (ロジスティック関数)
この2つの代わりに類似の
## 双曲線正接関数
を使うことがある.いずれの関数も,
入力の絶対値が大きな値をとると出力が飽和し一定値となること,その間の入力に対して出力が徐々に,かつ,滑らかに変化することが特徴であり,一般に
## sigmoid function (シグモイド関数)
と総称される.これらは生物の神経細胞が持つ性質をモデル化したもの.
近年,上記の関数に代わり
# retified linear function あるいは単にrectifier (正規化線形関数)
がよく使われている.なお,この関数を持つユニットのことを ReLU (Rectified Linear Unit) と略記することがある.

以上の関数が最も標準なものだが,これらの他にもいくつかの activation function がある.
線形写像や恒等写像,
クラス分類を目的とするネットワークでは,出力層の活性化関数に通常,ソフトマックス関数を使う.
さらに,rectifierともつながりの深い maxout (マックスアウト)と呼ばれる関数がある.
