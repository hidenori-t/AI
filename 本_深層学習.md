# 参考文献
岡谷貴之 :『深層学習』講談社, 2015.

# 1.1.2
# autoencoder (自己符号化器)
入力に対し計算される出力が,入力になるべく近くなるように訓練されるニューラルネット.

# 2.1
# feedforward neural network : FFNN (順伝播型ニューラルネットワーク)
層状に並べたユニットが隣接層間でのみ結合した構造を持ち,情報が入力側から出力側に一方向にのみ伝播するニューラルネットワーク.
文献によっては multi-layer perceptron(多層パーセプトロン)と呼ばれる.  

第1層のユニットの出力から第2層のユニットの出力が決まるまでの計算は,次のように一般化される. ( j = 1,...,J ).  

$$ u_{j} = \sum_{i=1}^{I} w_{ji}x_{i} + b_{j} $$

$$ z_{j} = f(u_{j}) $$  

各ベクトルと行列を次の様に定義する.  

$ \mathbf{u} =
\begin{bmatrix} u_1 & \ldots & u_J \end{bmatrix}
^{\mathrm{T}}
$,  

$ \mathbf{x} =  \begin{bmatrix} x_1 & \ldots & x_I \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{b} =  \begin{bmatrix} b_1 & \ldots & b_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{z} =  \begin{bmatrix} z_1 & \ldots & z_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{W} = \left[
    \begin{array}{rrr}
      w_{11} & \ldots & w_{1I} \\
      \vdots & \ddots & \vdots \\
      w_{J1} & \ldots & w_{JI}
    \end{array}
  \right]
$ ,

$ \mathbf{f}(\mathbf{u}) =  \begin{bmatrix} f(u_1) & \ldots & f(u_J) \end{bmatrix}^{\mathrm{T}} $,

すると, FFNN は

$$ \mathbf{u} = \mathbf{W}_{\mathbf{X}} + \mathbf{b} $$

$$ \mathbf{z} = \mathbf{f}(\mathbf{u}) $$

と書かれる.

# 2.2 activation function (活性化関数)
古くから最もよく使われているのが,
## logistic sigmoid function (ロジスティックシグモイド関数) あるいは logistic function (ロジスティック関数)
この2つの代わりに類似の
## 双曲線正接関数
を使うことがある.いずれの関数も,
入力の絶対値が大きな値をとると出力が飽和し一定値となること,その間の入力に対して出力が徐々に,かつ,滑らかに変化することが特徴であり,一般に
## sigmoid function (シグモイド関数)
と総称される.これらは生物の神経細胞が持つ性質をモデル化したもの.
近年,上記の関数に代わり
# retified linear function あるいは単にrectifier (正規化線形関数)
がよく使われている.なお,この関数を持つユニットのことを ReLU (Rectified Linear Unit) と略記することがある.

以上の関数が最も標準なものだが,これらの他にもいくつかの activation function がある.
線形写像や恒等写像,
クラス分類を目的とするネットワークでは,出力層の活性化関数に通常,ソフトマックス関数を使う.
さらに,rectifierともつながりの深い maxout (マックスアウト)と呼ばれる関数がある.

ちなみに,chainerでも retified linear functionと,誤差の出力でソフトマックス関数が使われている.
```
F.relu(model.l1(x))
```
```
return F.softmax_cross_entropy(y, t), F.accuracy(y, t)
```
Chainerのコードで言うと  
https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py  
にある
```
def forward_cpu(self, inputs):
        x, t = inputs
        self.y, = Softmax().forward_cpu((x,))
        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,
```
に相当  
[【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)

Overfitting (過学習) 細部を見過ぎて,一般性を失っている状態
未知を扱うので過学習は望ましくない.
過学習回避手法 Dropout
