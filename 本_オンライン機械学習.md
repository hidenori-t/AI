# 参考文献
海野 裕也  (著), 岡野原 大輔  (著), 得居 誠也  (著), 徳永 拓之  (著):『[オンライン機械学習 (機械学習プロフェッショナルシリーズ)](http://amzn.to/1Uw0EE3)』講談社, 2015.

# 1.1 機械学習とは
機械自身が例を通じて学習する手法.

## 特徴
推定の材料に使えそうな材料.
機械学習は推定に使えそうなものならなんでも使う.  


# 1.4 オンライン学習の短所
学習するデータの順番に学習結果が大きく依存する点  
例えば学習データの後半に同じ正解ラベルのデータばかりが出現する場合,学習モデルはそのラベルばかりを予測するようになってしまう.  
それはオンライン学習はデータの変化に追従しているとみることもできる.

<!-- それは,ヒトの,文脈を読むような,思い込みなのでは? -->
<!-- 偏微分 -->

# 2.2.10 計算量について

計算量を $O(n^{2})$ と書く.
このように$O$(計算量の最も大きな項)という書き方を$O$-記法という.

# Chapter 3

# 3つの定番のアルゴリズム
* パーセプトロン
* サポートベクトルマシン
* ロジスティック回帰

パーセプトロンとsvmの間にある種の類似性が存在する

サポートベクトルマシンやロジスティック回帰のように目的関数を定めるタイプの学習アルゴリズムでは,パラメータの最適化アルゴリズムは別途必要になる

# 3.1 二値分類
## 分類器
分類を行うプログラム,もしくは関数のこと

## 二値分類
データを2つのクラス(カテゴリなどともいいます)に分類する問題のこと  
たいていの場合に「いいかんじに」分類してくれるような $f(\mathbf{x})$ を学習することが,二値分類器における学習のテーマ

# 3.2 線形分類器 (linear classifier)
## 線形分類器 (linear classifier)
関数 $f(x)$ を次の形で書けるものを言う.

$$ f(x) = \mathbf{w^{\mathrm{T}}x} +b $$

$\mathbf{w}  \in  R^{m-1}$ は,重みベクトルといい,  
$b  \in R$ をバイアス項という.  
$\mathbf{w}\ ,b$ をまとめてパラメータという.

パラメータ $\mathbf{w}\ ,b$ を適切な値に調整することができれば,二値分類が解けることになる.  
データを使ってバラメータを適切に調整することを,学習と言う.

# 3.3 パーセプトロン (perceptron)
多くの場合,パーセプトロンよりも性能が良いアルゴリズムが存在する.

# 3.4 目的関数と最適化手法

## 目的関数
達成したい目的をあらわした関数
## 最適化手法
目的を達成するための方法


















***
# 1.1.2
# autoencoder (自己符号化器)
入力に対し計算される出力が,入力になるべく近くなるように訓練されるニューラルネット.

# 2.1
# feedforward neural network : FFNN (順伝播型ニューラルネットワーク)
層状に並べたユニットが隣接層間でのみ結合した構造を持ち,情報が入力側から出力側に一方向にのみ伝播するニューラルネットワーク.
文献によっては multi-layer perceptron,MLPs (多層パーセプトロン)と呼ばれる.  

第1層のユニットの出力から第2層のユニットの出力が決まるまでの計算は,次のように一般化される. ( j = 1,...,J ).  

$$ u_{j} = \sum_{i=1}^{I} w_{ji}x_{i} + b_{j} $$

$$ z_{j} = f(u_{j}) $$  

各ベクトルと行列を次の様に定義する.  

$ \mathbf{u} =
\begin{bmatrix} u_1 & \ldots & u_J \end{bmatrix}
^{\mathrm{T}}
$,  

$ \mathbf{x} =  \begin{bmatrix} x_1 & \ldots & x_I \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{b} =  \begin{bmatrix} b_1 & \ldots & b_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{z} =  \begin{bmatrix} z_1 & \ldots & z_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{W} = \left[
    \begin{array}{rrr}
      w_{11} & \ldots & w_{1I} \\
      \vdots & \ddots & \vdots \\
      w_{J1} & \ldots & w_{JI}
    \end{array}
  \right]
$ ,

$ \mathbf{f}(\mathbf{u}) =  \begin{bmatrix} f(u_1) & \ldots & f(u_J) \end{bmatrix}^{\mathrm{T}} $,

すると, FFNN は

$$ \mathbf{u} = \mathbf{W}_{\mathbf{X}} + \mathbf{b} $$

$$ \mathbf{z} = \mathbf{f}(\mathbf{u}) $$

と書かれる.

***
"バイアスは、パーセプトロンが1を出力する傾向の高さを表す量だとみなすことができます。  
あるいは、生物学の例えを使えば、バイアスとは、パーセプトロンというニューロンが発火 する傾向の高さを表すといえます。"  
[CHAPTER 1ニューラルネットワークを用いた手書き文字認識](http://nnadl-ja.github.io/nnadl_site_ja/chap1.html)

***
[Deep learning実装の基礎と実践 6,7page ](http://www.slideshare.net/beam2d/deep-learningimplementation)より
![Deep learning実装の基礎と実践](http://image.slidesharecdn.com/deep-learning-implementation-140826205939-phpapp02/95/deep-learning-6-638.jpg?cb=1409086999)

$$ h_{j} = f(w_{j1}x_{1} + \cdots + w_{j4}x_{4} + b_{j}) $$
$$       = f(w_{j}^{\mathrm{T}}x + b_{j}) $$

![Deep learning実装の基礎と実践](http://image.slidesharecdn.com/deep-learning-implementation-140826205939-phpapp02/95/deep-learning-7-638.jpg?cb=1409086999)

$$ h = f(Wx + b) $$
$$ y = f_{3} (W_{3} f_{2} (W_{2} f_{1}(W_{1}x + b_{1}) + b_{2}) + b_{3}) $$

# 2.2 activation function (活性化関数)
古くから最もよく使われているのが,

## logistic sigmoid function (ロジスティックシグモイド関数) あるいは logistic function (ロジスティック関数)
$$
  f(u) = \frac{1}{1+e^{-u}}.
$$

代わりに類似の

## 双曲線正接関数
を使うことがある.いずれの関数も,
入力の絶対値が大きな値をとると出力が飽和し一定値となること,その間の入力に対して出力が徐々に,かつ,滑らかに変化することが特徴であり,一般に

## sigmoid function (シグモイド関数) = $\sigma$
と総称される.これらは生物の神経細胞が持つ性質をモデル化したもの.
近年,上記の関数に代わり

# retified linear function あるいは単にrectifier (正規化線形関数)
がよく使われている.なお,この関数を持つユニットのことを ReLU (Rectified Linear Unit) と略記することがある.

以上の関数が最も標準なものだが,これらの他にもいくつかの activation function がある.
線形写像や恒等写像,
クラス分類を目的とするネットワークでは,出力層の活性化関数に通常,ソフトマックス関数を使う.
さらに,rectifierともつながりの深い maxout (マックスアウト)と呼ばれる関数がある.

ちなみに,chainerでも retified linear functionと,誤差の出力でソフトマックス関数が使われている.
```
F.relu(model.l1(x))
```
```
return F.softmax_cross_entropy(y, t), F.accuracy(y, t)
```
Chainerのコードで言うと  
https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py  
にある
```
def forward_cpu(self, inputs):
        x, t = inputs
        self.y, = Softmax().forward_cpu((x,))
        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,
```
に相当  
[【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)

Overfitting (過学習) 細部を見過ぎて,一般性を失っている状態
未知を扱うので過学習は望ましくない.
過学習回避手法 Dropout
