# 参考文献
海野 裕也  (著), 岡野原 大輔  (著), 得居 誠也  (著), 徳永 拓之  (著):『[オンライン機械学習 (機械学習プロフェッショナルシリーズ)](http://amzn.to/1Uw0EE3)』講談社, 2015.

# 1.1 機械学習とは
機械自身が例を通じて学習する手法.

## 特徴
推定の材料に使えそうな材料.
機械学習は推定に使えそうなものならなんでも使う.  


# 1.4 オンライン学習の短所
学習するデータの順番に学習結果が大きく依存する点  
例えば学習データの後半に同じ正解ラベルのデータばかりが出現する場合,学習モデルはそのラベルばかりを予測するようになってしまう.  
それはオンライン学習はデータの変化に追従しているとみることもできる.

<!-- それは,ヒトの,文脈を読むような,思い込みなのでは? -->
<!-- 偏微分 -->
# 2.1 数式を読む際の心構え
* 式を読む際には, その"式の型"を考えることを心がける

  型とは,例えば整数だとか,整数のベクトルだとか,実数だとか,そういうもの.
  プログラムを実装する際,それぞれの関数の型を気をつけると,実装やデバックが楽になる.

  書いているプログラムの型がどこかでずれていたら,ずれている付近に誤りがあるはずなので,その周辺を調べればいいとすぐにわかる.

* 数式が理解できずに悩んだ場合は数式は完璧なものだと思わず,ある程度想像力を働かせて読むことをお勧めします

# 2.2.2 ベクトル
ベクトルという言葉を単語を全て配列と読み替えてもかまわない.
ベクトルは数値の列を塊として扱うものなので, その中身をよく見ると,
$$(x_{1},\cdot\cdot\cdot,x_{n})$$
のようになる.(長さが$n$の場合)

$m$次元の実数値ベクトル全体を
$$R^{m}$$
と書き,  
$\mathbf{x}$ が$m$次元の実数値ベクトルであることを表すものとする.

ベクトルの内積は
$$ \mathbf{w^{\mathrm{T}}x}$$
と表現する.


内積の計算は数式で書くと,
$$ x_{1}y_{1}+x_{2}y_{2}+\cdot\cdot\cdot+x_{n}y_{n} $$
のようになる.  
ベクトルの内積の結果は実数になる.

# 2.2.7 指数関数
ネイピア数 $e$ (約2.718)を低とする指数関数$e^{x}$ を, $exp(x)$ と書く.

# 2.2.8 偏微分と勾配
偏微分とは,多変数関数を,注目している変数以外は定数とみなして微分すること.

<!-- 微分の復習が必要-->

関数の勾配とは,関数を変数で偏微分した結果を並べてベクトルにしたもの


# 2.2.10 計算量について

計算量を $O(n^{2})$ と書く.
このように$O$(計算量の最も大きな項)という書き方を$O$-記法という.

# Chapter 3

# 3つの定番のアルゴリズム
* パーセプトロン
* サポートベクトルマシン
* ロジスティック回帰

パーセプトロンとsvmの間にある種の類似性が存在する

サポートベクトルマシンやロジスティック回帰のように目的関数を定めるタイプの学習アルゴリズムでは,パラメータの最適化アルゴリズムは別途必要になる

# 3.1 二値分類
## 分類器
分類を行うプログラム,もしくは関数のこと

## 二値分類
データを2つのクラス(カテゴリなどともいいます)に分類する問題のこと  
たいていの場合に「いいかんじに」分類してくれるような $f(\mathbf{x})$ を学習することが,二値分類器における学習のテーマ

# 3.2 線形分類器 (linear classifier)
## 線形分類器 (linear classifier)
関数 $f(x)$ を次の形で書けるものを言う.

$$ f(x) = \mathbf{w^{\mathrm{T}}x} +b $$

$\mathbf{w}  \in  R^{m-1}$ は,重みベクトルといい,  
$b  \in R$ をバイアス項という.  
$\mathbf{w}\ ,b$ をまとめてパラメータという.

パラメータ $\mathbf{w}\ ,b$ を適切な値に調整することができれば,二値分類が解けることになる.  
データを使ってバラメータを適切に調整することを,学習と言う.

# 3.3 パーセプトロン (perceptron)
多くの場合,パーセプトロンよりも性能が良いアルゴリズムが存在する.

# 3.4 目的関数と最適化手法

## 目的関数
達成したい目的をあらわした関数
## 最適化手法
目的を達成するための方法

# 3.5 確率的勾配降下法 (stochastic gradient descent)
オンライン学習の最も基礎的な最適化手法
目的関数の最小化を目的とする場合に用いる勾配法の一種

## 3.5.1 勾配法
機械学習の文脈において,パラメータ $\mathbf{w}$ を引数としてとる目的関数 $L\mathbf{(w)}$ の値を最小化するための手法

## 3.5.3 確率的勾配降下法
勾配法 の一種で,前項の勾配降下法 をオンライン学習化したような手法

確率的勾配降下法では, 1つのデータを読み込んだら,そのデータのみを使って勾配を計算し,パラメータを更新する.
ミニバッチやモーメンタム法は,勾配をより上手く近似するための手法であるといえる
***








# 1.1.2
# autoencoder (自己符号化器)
入力に対し計算される出力が,入力になるべく近くなるように訓練されるニューラルネット.

# 2.1
# feedforward neural network : FFNN (順伝播型ニューラルネットワーク)
層状に並べたユニットが隣接層間でのみ結合した構造を持ち,情報が入力側から出力側に一方向にのみ伝播するニューラルネットワーク.
文献によっては multi-layer perceptron,MLPs (多層パーセプトロン)と呼ばれる.  

第1層のユニットの出力から第2層のユニットの出力が決まるまでの計算は,次のように一般化される. ( j = 1,...,J ).  

$$ u_{j} = \sum_{i=1}^{I} w_{ji}x_{i} + b_{j} $$

$$ z_{j} = f(u_{j}) $$  

各ベクトルと行列を次の様に定義する.  

$ \mathbf{u} =
\begin{bmatrix} u_1 & \ldots & u_J \end{bmatrix}
^{\mathrm{T}}
$,  

$ \mathbf{x} =  \begin{bmatrix} x_1 & \ldots & x_I \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{b} =  \begin{bmatrix} b_1 & \ldots & b_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{z} =  \begin{bmatrix} z_1 & \ldots & z_J \end{bmatrix}^{\mathrm{T}} $,

$ \mathbf{W} = \left[
    \begin{array}{rrr}
      w_{11} & \ldots & w_{1I} \\
      \vdots & \ddots & \vdots \\
      w_{J1} & \ldots & w_{JI}
    \end{array}
  \right]
$ ,

$ \mathbf{f}(\mathbf{u}) =  \begin{bmatrix} f(u_1) & \ldots & f(u_J) \end{bmatrix}^{\mathrm{T}} $,

すると, FFNN は

$$ \mathbf{u} = \mathbf{W}_{\mathbf{X}} + \mathbf{b} $$

$$ \mathbf{z} = \mathbf{f}(\mathbf{u}) $$

と書かれる.

***
"バイアスは、パーセプトロンが1を出力する傾向の高さを表す量だとみなすことができます。  
あるいは、生物学の例えを使えば、バイアスとは、パーセプトロンというニューロンが発火 する傾向の高さを表すといえます。"  
[CHAPTER 1ニューラルネットワークを用いた手書き文字認識](http://nnadl-ja.github.io/nnadl_site_ja/chap1.html)

***
[Deep learning実装の基礎と実践 6,7page ](http://www.slideshare.net/beam2d/deep-learningimplementation)より
![Deep learning実装の基礎と実践](http://image.slidesharecdn.com/deep-learning-implementation-140826205939-phpapp02/95/deep-learning-6-638.jpg?cb=1409086999)

$$ h_{j} = f(w_{j1}x_{1} + \cdots + w_{j4}x_{4} + b_{j}) $$
$$       = f(w_{j}^{\mathrm{T}}x + b_{j}) $$

![Deep learning実装の基礎と実践](http://image.slidesharecdn.com/deep-learning-implementation-140826205939-phpapp02/95/deep-learning-7-638.jpg?cb=1409086999)

$$ h = f(Wx + b) $$
$$ y = f_{3} (W_{3} f_{2} (W_{2} f_{1}(W_{1}x + b_{1}) + b_{2}) + b_{3}) $$

# 2.2 activation function (活性化関数)
古くから最もよく使われているのが,

## logistic sigmoid function (ロジスティックシグモイド関数) あるいは logistic function (ロジスティック関数)
$$
  f(u) = \frac{1}{1+e^{-u}}.
$$

代わりに類似の

## 双曲線正接関数
を使うことがある.いずれの関数も,
入力の絶対値が大きな値をとると出力が飽和し一定値となること,その間の入力に対して出力が徐々に,かつ,滑らかに変化することが特徴であり,一般に

## sigmoid function (シグモイド関数) = $\sigma$
と総称される.これらは生物の神経細胞が持つ性質をモデル化したもの.
近年,上記の関数に代わり

# retified linear function あるいは単にrectifier (正規化線形関数)
がよく使われている.なお,この関数を持つユニットのことを ReLU (Rectified Linear Unit) と略記することがある.

以上の関数が最も標準なものだが,これらの他にもいくつかの activation function がある.
線形写像や恒等写像,
クラス分類を目的とするネットワークでは,出力層の活性化関数に通常,ソフトマックス関数を使う.
さらに,rectifierともつながりの深い maxout (マックスアウト)と呼ばれる関数がある.

ちなみに,chainerでも retified linear functionと,誤差の出力でソフトマックス関数が使われている.
```
F.relu(model.l1(x))
```
```
return F.softmax_cross_entropy(y, t), F.accuracy(y, t)
```
Chainerのコードで言うと  
https://github.com/pfnet/chainer/blob/master/chainer/functions/softmax_cross_entropy.py  
にある
```
def forward_cpu(self, inputs):
        x, t = inputs
        self.y, = Softmax().forward_cpu((x,))
        return -numpy.log(self.y[xrange(len(t)), t]).sum(keepdims=True) / t.size,
```
に相当  
[【機械学習】ディープラーニング フレームワークChainerを試しながら解説してみる。](http://qiita.com/kenmatsu4/items/7b8d24d4c5144a686412)

Overfitting (過学習) 細部を見過ぎて,一般性を失っている状態
未知を扱うので過学習は望ましくない.
過学習回避手法 Dropout
